{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Exkaldi\n",
    "\n",
    "In this section, we will training a DNN acoustic model with __Tensorflow 2.x__.\n",
    "\n",
    "If you want run this step, please install Tensorflow firstly.  \n",
    "In this tutorial, we will customize the training loop with out using \"__fit__\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exkaldi\n",
    "\n",
    "import os\n",
    "dataDir = \"librispeech_dummy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use keras to build and train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "Restorage the training feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featFile = os.path.join(dataDir, \"exp\", \"train_mfcc_cmvn.ark\")\n",
    "feat = exkaldi.load_feat(featFile)\n",
    "feat = feat.add_delta(order=2)\n",
    "feat = feat.splice(left=1,right=1)\n",
    "feat = feat.to_numpy()\n",
    "\n",
    "feat.dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___feat___ is an exkaldi __NumpyFeature__ object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature is made following these steps:\n",
    "\n",
    "    compute mfcc (13) >> apply CMVN (13) >> add 2 order deltas (39) >> splice 1-1 frames (117)\n",
    "\n",
    "We still further do global standerd normalization on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = feat.normalize(std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Them we load the alignment data. They have been generated in early step (07_train_triphone_HMM-GMM_delta).\n",
    "\n",
    "We will use pdf-ID as target label. In exkaldi, transition-ID and phone-ID can also be extracted for mutiple tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliFile = os.path.join(dataDir, \"exp\", \"train_delta\", \"final.ali\")\n",
    "hmmFile = os.path.join(dataDir, \"exp\", \"train_delta\", \"final.mdl\")\n",
    "\n",
    "ali = exkaldi.load_ali(aliFile)\n",
    "\n",
    "ali = ali.to_numpy(aliType=\"pdfID\", hmm=hmmFile)\n",
    "\n",
    "ali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alignment will be label date to train the NN model.\n",
    "\n",
    "Then we tuple the feature and alignment in order to generate a dataset for deep learning framework. We use __tuple_data(...)__ function to group them. \n",
    "\n",
    "But note that, this function will group the archieves by their name, so please ensure their names are avaliable as python identifiers. (that means, we only allow lower and upper letters, digits, and underline in their names.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat.rename(\"mfcc\")\n",
    "ali.rename(\"pdfID\")\n",
    "\n",
    "dataset = exkaldi.tuple_data([feat,ali], frameLevel=True)\n",
    "\n",
    "datasetSize = len(dataset)\n",
    "datasetSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___dataset___ is a list. whose members are namedtuples. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneRecord = dataset[0]\n",
    "\n",
    "oneRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use name to get specified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneRecord.pdfID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you train a sequential NN model, you may not want to tuple archieves data in __frame level__ but in __utterance level__. try to change the mode of tuple. \n",
    "\n",
    "You can tuple all kinds of exkaldi archieves such as feature, CMVN, alignment, probability, transcription and so on. And even different feature such as MFCC and fBank, different alignment such as PdfID and Phone ID, can also be grouped. For example, now we want to do multiple tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ali2 = exkaldi.load_ali(aliFile)\n",
    "\n",
    "ali2 = ali2.to_numpy(aliType=\"phoneID\", hmm=hmmFile)\n",
    "\n",
    "ali2.rename(\"phoneID\")\n",
    "\n",
    "dataset2 = exkaldi.tuple_data([feat,ali,ali2], frameLevel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ali2\n",
    "del dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now we start to train DNN acoustic model. Fisrtly, design a data iterator from our provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDim = feat.dim\n",
    "pdfClasses = exkaldi.hmm.load_hmm(hmmFile,\"triphone\").info.pdfs\n",
    "\n",
    "del ali\n",
    "del feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generater(dataset):\n",
    "\n",
    "    length = len(dataset)\n",
    "    while True:\n",
    "        index = 0\n",
    "        random.shuffle(dataset)\n",
    "        while index < length:\n",
    "            one = dataset[index]\n",
    "            index += 1\n",
    "            yield (one.mfcc[0], one.pdfID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64\n",
    "tf_datasets = tf.data.Dataset.from_generator(\n",
    "                                 lambda : data_generater(dataset),\n",
    "                                 (tf.float32, tf.int32)\n",
    "                            ).batch(batchSize).prefetch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define a simple Dense model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DNN_model(inputsShape, classes):\n",
    "    \n",
    "    inputs = keras.Input(inputsShape)\n",
    "    h1 = keras.layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\")(inputs)\n",
    "    h1_bn = keras.layers.BatchNormalization()(h1)\n",
    "    \n",
    "    h2 = keras.layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\")(h1_bn)\n",
    "    h2_bn = keras.layers.BatchNormalization()(h2)\n",
    "    \n",
    "    h3 = keras.layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\")(h2_bn)\n",
    "    h3_bn = keras.layers.BatchNormalization()(h3)\n",
    "    \n",
    "    outputs = keras.layers.Dense(classes, use_bias=False)(h3_bn)\n",
    "    \n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_DNN_model((featureDim,), pdfClasses)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are optimizer and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "\n",
    "losses = keras.metrics.Mean(name=\"train/loss\", dtype=tf.float32)\n",
    "accs = keras.metrics.Mean(name=\"train/accuracy\", dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speecify the output dir. You can use tensorboard to check the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDir = os.path.join(dataDir, \"exp\", \"train_DNN\")\n",
    "\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logDir = os.path.join(outDir, \"log\", stamp)\n",
    "file_writer = tf.summary.create_file_writer(logDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "epoch_iterations = datasetSize//batchSize\n",
    "\n",
    "epoch_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to print the progress bar and control the epoch ending, we will lend a hand from __tqdm__ package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tqdm 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to train this model. During the training loop, You can use tensorboard to look the visiable training result.\n",
    "\n",
    "```\n",
    "tensorboard --logdir=./librispeech_dummy/exp/train_DNN/log --bind_all\n",
    "```\n",
    "\n",
    "Just for fun, we do not validate the model during the training, but in real case, you should do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with file_writer.as_default():\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for batch,i in zip(tf_datasets, tqdm(range(epoch_iterations))):\n",
    "            data, label = batch\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(data, training=True)\n",
    "                loss = keras.losses.sparse_categorical_crossentropy(label, logits, from_logits=True)\n",
    "                losses(loss)\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "                pred = keras.backend.argmax(logits, axis=1)\n",
    "\n",
    "                acc = exkaldi.nn.accuracy(label.numpy(), pred.numpy())\n",
    "                accs(acc.accuracy)\n",
    "        \n",
    "            #if int(optimizer.iterations.numpy()) % epoch_iterations == 0:     #<<<< if you don't use tqdm\n",
    "            #    break\n",
    "        \n",
    "        current_loss = losses.result()\n",
    "        current_acc = accs.result()\n",
    "        tf.print( f\"Epoch {epoch}\", f\" Loss {current_loss:.6f}\", f\" Acc {current_acc:.6f}\")\n",
    "\n",
    "        tf.summary.scalar(\"train/loss\", data=current_loss, step=epoch)\n",
    "        tf.summary.scalar(\"train/accuracy\", data=current_acc, step=epoch)\n",
    "\n",
    "    tf.print( \"Training Done\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model in file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfModelFile = os.path.join(outDir, \"dnn.h5\")\n",
    "\n",
    "model.save(tfModelFile, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we predict the network output for test data for decoding. We do the same processing as training feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeatFile = os.path.join(dataDir, \"exp\", \"test_mfcc_cmvn.ark\")\n",
    "testFeat = exkaldi.load_feat(testFeatFile)\n",
    "testFeat = testFeat.add_delta(order=2).splice(left=1,right=1)\n",
    "testFeat = testFeat.to_numpy()\n",
    "testFeat = testFeat.normalize(std=True)\n",
    "\n",
    "testFeat.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = {}\n",
    "for utt, mat in testFeat.items():\n",
    "    logits = model(mat, training=False)\n",
    "    prob[utt] = logits.numpy()\n",
    "\n",
    "prob = exkaldi.load_prob(prob)\n",
    "\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___prob___ is an exkaldi __NumpyProbability__ object. Save it to file. We will decode it in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probFile = os.path.join(outDir, \"amp.npy\")\n",
    "\n",
    "prob.save(probFile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
